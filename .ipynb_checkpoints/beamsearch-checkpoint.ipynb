{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "iris = load_iris()\n",
    "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
    "                               random_state=0)\n",
    "distributions = dict(C=uniform(loc=0, scale=4),\n",
    "                     penalty=['l2', 'l1'])\n",
    "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "search = clf.fit(iris.data, iris.target)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0r/095b68zx2zv0t279tgqqhl7w0000gn/T/ipykernel_27230/1974669932.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mPIL_Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnodeserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExpansionNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnd_ExpansionNet_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEnd_ExpansionNet_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnodeserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpythonscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExpansionNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_vector_idx2word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mCPU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import argparse\n",
    "import pickle\n",
    "from argparse import Namespace\n",
    "\n",
    "from PIL import Image as PIL_Image\n",
    "from .nodeserver.pythonscripts.ExpansionNet.models.End_ExpansionNet_v2 import End_ExpansionNet_v2\n",
    "from .nodeserver.pythonscripts.ExpansionNet.utils.language_utils import convert_vector_idx2word\n",
    "CPU = torch.device('cpu')\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "            N_dec:\n",
    "            3\n",
    "            N_enc:\n",
    "            3\n",
    "            beam_size:\n",
    "            5\n",
    "            image_paths:\n",
    "            ['./demo_material/tatin.jpg', './demo_material/micheal.jpg', './demo_material/napoleon.jpg', './demo_material/cat_girl.jpg']\n",
    "            load_path:\n",
    "            './rf_model.pth'\n",
    "            max_seq_len:\n",
    "            74\n",
    "            model_dim:\n",
    "            512\n",
    "    \n",
    "    '''\n",
    "    image_paths=['./nodeserver/pythonscripts/test/1176519574_b4d569ecbe_z.jpg','nodeserver/pythonscripts/test/2633528304_369cf89ce1_z.jpg']\n",
    "    drop_args = Namespace(enc=0.0,\n",
    "                          dec=0.0,\n",
    "                          enc_input=0.0,\n",
    "                          dec_input=0.0,\n",
    "                          other=0.0)\n",
    "    model_dim=512\n",
    "    N_enc=3\n",
    "    N_dec=3\n",
    "    max_seq_len=74\n",
    "    beam_size=5\n",
    "    load_path='./rf_model.pth'\n",
    "    model_args = Namespace(model_dim=model_dim,\n",
    "                           N_enc=N_enc,\n",
    "                           N_dec=N_dec,\n",
    "                           dropout=0.0,\n",
    "                           drop_args=drop_args)\n",
    "\n",
    "    with open('./demo_coco_tokens.pickle', 'rb') as f:\n",
    "        coco_tokens = pickle.load(f)\n",
    "    print(\"Dictionary loaded ...\")\n",
    "\n",
    "    img_size = 384\n",
    "    model = End_ExpansionNet_v2(swin_img_size=img_size, swin_patch_size=4, swin_in_chans=3,\n",
    "                                swin_embed_dim=192, swin_depths=[2, 2, 18, 2], swin_num_heads=[6, 12, 24, 48],\n",
    "                                swin_window_size=12, swin_mlp_ratio=4., swin_qkv_bias=True, swin_qk_scale=None,\n",
    "                                swin_drop_rate=0.0, swin_attn_drop_rate=0.0, swin_drop_path_rate=0.0,\n",
    "                                swin_norm_layer=torch.nn.LayerNorm, swin_ape=False, swin_patch_norm=True,\n",
    "                                swin_use_checkpoint=False,\n",
    "                                final_swin_dim=1536,\n",
    "\n",
    "                                d_model=model_args.model_dim, N_enc=model_args.N_enc,\n",
    "                                N_dec=model_args.N_dec, num_heads=8, ff=2048,\n",
    "                                num_exp_enc_list=[32, 64, 128, 256, 512],\n",
    "                                num_exp_dec=16,\n",
    "                                output_word2idx=coco_tokens['word2idx_dict'],\n",
    "                                output_idx2word=coco_tokens['idx2word_list'],\n",
    "                                max_seq_len=max_seq_len, drop_args=model_args.drop_args,\n",
    "                                rank=0)\n",
    "    model.to(CPU)#\n",
    "    map_location = {'cpu': 'cpu'}#\n",
    "    \n",
    "    checkpoint = torch.load(load_path, map_location=map_location)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Model loaded ...\")\n",
    "\n",
    "    transf_1 = torchvision.transforms.Compose([torchvision.transforms.Resize((img_size, img_size))])\n",
    "    transf_2 = torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    input_images = []\n",
    "    for path in image_paths:\n",
    "        pil_image = PIL_Image.open(path)\n",
    "        if pil_image.mode != 'RGB':\n",
    "           pil_image = PIL_Image.new(\"RGB\", pil_image.size)\n",
    "        preprocess_pil_image = transf_1(pil_image)\n",
    "        tens_image_1 = torchvision.transforms.ToTensor()(preprocess_pil_image)\n",
    "        tens_image_2 = transf_2(tens_image_1)\n",
    "        input_images.append(tens_image_2)\n",
    "\n",
    "    print(\"Generating captions ...\\n\")\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(len(input_images)):\n",
    "        path = image_paths[i]\n",
    "        image = input_images[i].unsqueeze(0).to(CPU)\n",
    "        beam_search_kwargs = {'beam_size': beam_size,\n",
    "                              'beam_max_seq_len': max_seq_len,\n",
    "                              'sample_or_max': 'max',\n",
    "                              'how_many_outputs': 1,\n",
    "                              'sos_idx': coco_tokens['word2idx_dict'][coco_tokens['sos_str']],\n",
    "                              'eos_idx': coco_tokens['word2idx_dict'][coco_tokens['eos_str']]}\n",
    "        with torch.no_grad():\n",
    "            pred, _ = model(enc_x=image,\n",
    "                            enc_x_num_pads=[0],\n",
    "                            mode='beam_search', **beam_search_kwargs)\n",
    "        pred = convert_vector_idx2word(pred[0][0], coco_tokens['idx2word_list'])[1:-1]\n",
    "        pred[-1] = pred[-1] + '.'\n",
    "        pred = ' '.join(pred).capitalize()\n",
    "        print(path + ') \\n\\tDescription: ' + pred + '\\n')\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "\n",
    "    print(\"Closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
