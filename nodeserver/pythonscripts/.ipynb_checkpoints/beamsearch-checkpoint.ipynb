{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "iris = load_iris()\n",
    "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
    "                               random_state=0)\n",
    "distributions = dict(C=uniform(loc=0, scale=4),\n",
    "                     penalty=['l2', 'l1'])\n",
    "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "search = clf.fit(iris.data, iris.target)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0r/095b68zx2zv0t279tgqqhl7w0000gn/T/ipykernel_27835/1594567804.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         loaded_storages[key] = torch.storage.TypedStorage(\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m             dtype=dtype)\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    167\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import argparse\n",
    "import pickle\n",
    "from argparse import Namespace\n",
    "\n",
    "from PIL import Image as PIL_Image\n",
    "from ExpansionNet.models.End_ExpansionNet_v2 import End_ExpansionNet_v2\n",
    "from ExpansionNet.utils.language_utils import convert_vector_idx2word\n",
    "# CPU = torch.device('cpu')\n",
    "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None, entry_length=67, temperature=1., stop_token: str = '.'):\n",
    "#     image_paths=['./nodeserver/pythonscripts/test/1176519574_b4d569ecbe_z.jpg','nodeserver/pythonscripts/test/2633528304_369cf89ce1_z.jpg']\n",
    "    drop_args = Namespace(enc=0.0,\n",
    "                          dec=0.0,\n",
    "                          enc_input=0.0,\n",
    "                          dec_input=0.0,\n",
    "                          other=0.0)\n",
    "    model_dim=512\n",
    "    N_enc=3\n",
    "    N_dec=3\n",
    "    max_seq_len=74\n",
    "    beam_size=5\n",
    "    load_path='./ExpansionNet/rf_model.pth'\n",
    "    model_args = Namespace(model_dim=model_dim,\n",
    "                           N_enc=N_enc,\n",
    "                           N_dec=N_dec,\n",
    "                           dropout=0.0,\n",
    "                           drop_args=drop_args)\n",
    "\n",
    "    with open('./ExpansionNet/demo_coco_tokens.pickle', 'rb') as f:\n",
    "        coco_tokens = pickle.load(f)\n",
    "    print(\"Dictionary loaded ...\")\n",
    "\n",
    "    img_size = 384\n",
    "    model = End_ExpansionNet_v2(swin_img_size=img_size, swin_patch_size=4, swin_in_chans=3,\n",
    "                                swin_embed_dim=192, swin_depths=[2, 2, 18, 2], swin_num_heads=[6, 12, 24, 48],\n",
    "                                swin_window_size=12, swin_mlp_ratio=4., swin_qkv_bias=True, swin_qk_scale=None,\n",
    "                                swin_drop_rate=0.0, swin_attn_drop_rate=0.0, swin_drop_path_rate=0.0,\n",
    "                                swin_norm_layer=torch.nn.LayerNorm, swin_ape=False, swin_patch_norm=True,\n",
    "                                swin_use_checkpoint=False,\n",
    "                                final_swin_dim=1536,\n",
    "\n",
    "                                d_model=model_args.model_dim, N_enc=model_args.N_enc,\n",
    "                                N_dec=model_args.N_dec, num_heads=8, ff=2048,\n",
    "                                num_exp_enc_list=[32, 64, 128, 256, 512],\n",
    "                                num_exp_dec=16,\n",
    "                                output_word2idx=coco_tokens['word2idx_dict'],\n",
    "                                output_idx2word=coco_tokens['idx2word_list'],\n",
    "                                max_seq_len=max_seq_len, drop_args=model_args.drop_args,\n",
    "                                rank=0)\n",
    "    model.to(0)#\n",
    "    map_location = {'cuda:%d' % 0: 'cuda:%d' % 0}#\n",
    "    \n",
    "    checkpoint = torch.load(load_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Model loaded ...\")\n",
    "\n",
    "    transf_1 = torchvision.transforms.Compose([torchvision.transforms.Resize((img_size, img_size))])\n",
    "    transf_2 = torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    input_images = []\n",
    "    for path in image_paths:\n",
    "        pil_image = PIL_Image.open(path)\n",
    "        if pil_image.mode != 'RGB':\n",
    "           pil_image = PIL_Image.new(\"RGB\", pil_image.size)\n",
    "        preprocess_pil_image = transf_1(pil_image)\n",
    "        tens_image_1 = torchvision.transforms.ToTensor()(preprocess_pil_image)\n",
    "        tens_image_2 = transf_2(tens_image_1)\n",
    "        input_images.append(tens_image_2)\n",
    "\n",
    "    print(\"Generating captions ...\\n\")\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(len(input_images)):\n",
    "        path = image_paths[i]\n",
    "        image = input_images[i].unsqueeze(0).to(0)\n",
    "        beam_search_kwargs = {'beam_size': beam_size,\n",
    "                              'beam_max_seq_len': max_seq_len,\n",
    "                              'sample_or_max': 'max',\n",
    "                              'how_many_outputs': 1,\n",
    "                              'sos_idx': coco_tokens['word2idx_dict'][coco_tokens['sos_str']],\n",
    "                              'eos_idx': coco_tokens['word2idx_dict'][coco_tokens['eos_str']]}\n",
    "        with torch.no_grad():\n",
    "            pred, _ = model(enc_x=image,\n",
    "                            enc_x_num_pads=[0],\n",
    "                            mode='beam_search', **beam_search_kwargs)\n",
    "        pred = convert_vector_idx2word(pred[0][0], coco_tokens['idx2word_list'])[1:-1]\n",
    "        pred[-1] = pred[-1] + '.'\n",
    "        pred = ' '.join(pred).capitalize()\n",
    "        print(path + ') \\n\\tDescription: ' + pred + '\\n')\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "\n",
    "    print(\"Closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import os\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "import argparse\n",
    "from typing import Tuple, List, Union, Optional\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import skimage.io as io\n",
    "import PIL.Image\n",
    "from IPython.display import Image \n",
    "N = type(None)\n",
    "V = np.array\n",
    "ARRAY = np.ndarray\n",
    "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
    "VS = Union[Tuple[V, ...], List[V]]\n",
    "VN = Union[V, N]\n",
    "VNS = Union[VS, N]\n",
    "T = torch.Tensor\n",
    "TS = Union[Tuple[T, ...], List[T]]\n",
    "TN = Optional[T]\n",
    "TNS = Union[Tuple[TN, ...], List[TN]]\n",
    "TSN = Optional[TS]\n",
    "TA = Union[T, ARRAY]\n",
    "\n",
    "\n",
    "D = torch.device\n",
    "CPU = torch.device('cpu')\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def forward(self, x: T) -> T:\n",
    "        return self.model(x)\n",
    "\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) -1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    #@functools.lru_cache #FIXME\n",
    "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
    "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        if prefix_length > 10:  # not enough memory\n",
    "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
    "        else:\n",
    "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
    "\n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self\n",
    "\n",
    "\n",
    "#@title Caption prediction\n",
    "\n",
    "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
    "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
    "\n",
    "    model.eval()\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    tokens = None\n",
    "    scores = None\n",
    "    device = next(model.parameters()).device\n",
    "    seq_lengths = torch.ones(beam_size, device=device)\n",
    "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
    "    with torch.no_grad():\n",
    "        if embed is not None:\n",
    "            generated = embed\n",
    "        else:\n",
    "            if tokens is None:\n",
    "                tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                tokens = tokens.unsqueeze(0).to(device)\n",
    "                generated = model.gpt.transformer.wte(tokens)\n",
    "        for i in range(entry_length):\n",
    "            outputs = model.gpt(inputs_embeds=generated)\n",
    "            logits = outputs.logits\n",
    "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "            logits = logits.softmax(-1).log()\n",
    "            if scores is None:\n",
    "                scores, next_tokens = logits.topk(beam_size, -1)\n",
    "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
    "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
    "                if tokens is None:\n",
    "                    tokens = next_tokens\n",
    "                else:\n",
    "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
    "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "            else:\n",
    "                logits[is_stopped] = -float(np.inf)\n",
    "                logits[is_stopped, 0] = 0\n",
    "                scores_sum = scores[:, None] + logits\n",
    "                seq_lengths[~is_stopped] += 1\n",
    "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
    "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
    "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
    "                seq_lengths = seq_lengths[next_tokens_source]\n",
    "                next_tokens = next_tokens % scores_sum.shape[1]\n",
    "                next_tokens = next_tokens.unsqueeze(1)\n",
    "                tokens = tokens[next_tokens_source]\n",
    "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "                generated = generated[next_tokens_source]\n",
    "                scores = scores_sum_average * seq_lengths\n",
    "                is_stopped = is_stopped[next_tokens_source]\n",
    "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
    "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
    "            if is_stopped.all():\n",
    "                break\n",
    "    scores = scores / seq_lengths\n",
    "    output_list = tokens.cpu().numpy()\n",
    "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
    "    order = scores.argsort(descending=True)\n",
    "    output_texts = [output_texts[i] for i in order]\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "def generate2(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        tokens=None,\n",
    "        prompt=None,\n",
    "        embed=None,\n",
    "        entry_count=1,\n",
    "        entry_length=67,\n",
    "        top_p=0.8,\n",
    "        temperature=1.,\n",
    "        stop_token: str = '.',\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    filter_value = -float(\"Inf\")\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "            if embed is not None:\n",
    "                generated = embed\n",
    "            else:\n",
    "                if tokens is None:\n",
    "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                    tokens = tokens.unsqueeze(0).to(device)\n",
    "\n",
    "                generated = model.gpt.transformer.wte(tokens)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "\n",
    "                outputs = model.gpt(inputs_embeds=generated)\n",
    "                logits = outputs.logits\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                                                    ..., :-1\n",
    "                                                    ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
    "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
    "                if tokens is None:\n",
    "                    tokens = next_token\n",
    "                else:\n",
    "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
    "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "                if stop_token_index == next_token.item():\n",
    "                    break\n",
    "\n",
    "            output_list = list(tokens.squeeze().cpu().numpy())\n",
    "            output_text = tokenizer.decode(output_list)\n",
    "            generated_list.append(output_text)\n",
    "\n",
    "    return generated_list[0]\n",
    "\n",
    "\n",
    "def img_caption(filepath,beam_size):\n",
    "    print(filepath, file=sys.stderr)\n",
    "    def get_device(device_id: int) -> D:\n",
    "        if not torch.cuda.is_available():\n",
    "            return CPU\n",
    "        device_id = min(torch.cuda.device_count() - 1, device_id)\n",
    "        return torch.device(f'cuda:{device_id}')\n",
    "\n",
    "    is_gpu = True\n",
    "    CUDA = get_device\n",
    " #     model_path = os.path.join(os.getcwd(),\"nodeserver\",\"pythonscripts\",\"imagecaption\",\"model\", 'model_weights.pt')\n",
    "    model_path =\"./imagecaption/model/model_weights.pt\"\n",
    "    device = CUDA(0) if is_gpu else \"cpu\"\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    prefix_length = 10\n",
    "\n",
    "    model = ClipCaptionModel(prefix_length)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=CPU)) \n",
    "\n",
    "    model = model.eval() \n",
    "    device = CUDA(0) if is_gpu else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    use_beam_search = True  \n",
    "    image = io.imread(filepath)\n",
    "    pil_image = PIL.Image.fromarray(image)\n",
    "\n",
    "    # display(pil_image)\n",
    "    import time\n",
    "    start=time.time()\n",
    "    image = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
    "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "    if use_beam_search:\n",
    "        generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed,beam_size = beam_size)[0]\n",
    "    else:\n",
    "        generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
    "\n",
    "    end=time.time()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     print('\\n')\n",
    "    print(\"Time Taken\",str(end-start), file=sys.stderr)\n",
    "\n",
    "    print(generated_text_prefix, file=sys.stderr)\n",
    "    return end-start,str(generated_text_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.36925172805786133\n",
      "elephants walking in the bush.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 0.5059001445770264\n",
      "tennis player celebrates after winning his match against tennis player during day.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.2357950210571289\n",
      "a city is a port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 0.485889196395874\n",
      "person, left, and person, right, look at a mirror.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.29430699348449707\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.28139495849609375\n",
      "a bus is loaded with passengers.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.2283341884613037\n",
      "the train arrives at station.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 0.31842613220214844\n",
      "a train passes through australian suburb.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.28358006477355957\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.28292417526245117\n",
      "airliner on final approach to land.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.5232357978820801\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 0.6714208126068115\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.25226402282714844\n",
      "a view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 0.6032328605651855\n",
      "person, left, and person look at each other in the bathroom.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.33213114738464355\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.3708047866821289\n",
      "people boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.2558720111846924\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 0.41803717613220215\n",
      "a train passes through australian suburb.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.3613309860229492\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.8864119052886963\n",
      "aircraft model is a twin - turboprop aircraft manufactured by aircraft manufacturer.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.4702467918395996\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 0.773474931716919\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.493664026260376\n",
      "a view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 0.6643319129943848\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.3516077995300293\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.4826171398162842\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.2799248695373535\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 0.5868620872497559\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.36072206497192383\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.39345526695251465\n",
      "aircraft model in flight at the airport.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.5232391357421875\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 0.951495885848999\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.5288188457489014\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 0.9980430603027344\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.4111459255218506\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.5759768486022949\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.32153820991516113\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 0.6189398765563965\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.3997318744659424\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.4675750732421875\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.7044563293457031\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 1.2808513641357422\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.6074860095977783\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 1.0983312129974365\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.45165181159973145\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.5871312618255615\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.4036240577697754\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 0.7728328704833984\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.5357658863067627\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.5812902450561523\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.8522889614105225\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 1.4636850357055664\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.7450249195098877\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 1.5236008167266846\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.515070915222168\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.6167910099029541\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.4205777645111084\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 1.0254967212677002\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.5201637744903564\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.6247169971466064\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.829740047454834\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 1.6282217502593994\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.8039901256561279\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 1.7377619743347168\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.685150146484375\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.9143500328063965\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.5181550979614258\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 1.4427659511566162\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 1.054508924484253\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.6878409385681152\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.8629229068756104\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 1.4544460773468018\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.848242998123169\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 1.661358118057251\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.7598819732666016\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 1.1226201057434082\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.5131378173828125\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Time Taken 1.4412500858306885\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.8350081443786621\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.6843559741973877\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.9257407188415527\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 1.3441076278686523\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.718519926071167\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 2.3882830142974854\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.7126431465148926\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 1.2024767398834229\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.5763750076293945\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 1.6676709651947021\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.7873401641845703\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.6625909805297852\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 1.13572096824646\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 1.7635741233825684\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.6697781085968018\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 2.278876781463623\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.8613357543945312\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 1.2141938209533691\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.5897212028503418\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 1.5877108573913574\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 1.8266918659210205\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.7224709987640381\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 1.0622889995574951\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 1.939107894897461\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.8095560073852539\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 2.652601718902588\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.8945081233978271\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 1.1656692028045654\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.6382629871368408\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 1.8226351737976074\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 2.066473960876465\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.8160529136657715\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 1.1193270683288574\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 2.266441822052002\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 1.0560510158538818\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 3.1793060302734375\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 1.0165021419525146\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 1.290653944015503\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.5980591773986816\n",
      "a train for english civil parish.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 1.9111509323120117\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 1.9789419174194336\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.7747557163238525\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 1.1597089767456055\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 2.1681511402130127\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 1.0724620819091797\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 2.684414863586426\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.9194409847259521\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 1.255753755569458\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.6697452068328857\n",
      "a train for english civil parish.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 1.8676059246063232\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 2.339508295059204\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.8777940273284912\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 1.3157250881195068\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 2.3682260513305664\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 1.0315310955047607\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 3.012209892272949\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 1.029775857925415\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 1.3834319114685059\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.7110650539398193\n",
      "a train for english civil parish.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 2.0418381690979004\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 2.280606985092163\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 1.1373310089111328\n",
      "aircraft model on the runway.\n"
     ]
    }
   ],
   "source": [
    "ACTUALS=[\"two elephants walking in a grassy flat area\",\n",
    "            \"a tennis player getting ready to hit a ball\",\n",
    "            \"a group of large boats is parts at a city marina\",\n",
    "            \"a man standing next to a man in a bathroom\",\n",
    "            \"a herd of cattle are standing and grazing in a field\",\n",
    "            \"men standing in a street next to a car and a large bus\",\n",
    "            \"a train moves past a cement station platform\",\n",
    "            \"a long railroad train carrying several cars behind it\",\n",
    "            \"a man on a motor bike on a street\",\n",
    "            \"a jumbo jet airplane coming in for a landing on a runway\"]\n",
    "\n",
    "image_paths=['5155093236_01067e4f6e_z.jpg','2633528304_369cf89ce1_z.jpg','9874637964_ce338575f0_z.jpg','1176519574_b4d569ecbe_z.jpg','5093961450_d8c840d0d2_z.jpg','9372536208_b97bc7eeff_z.jpg','9419081044_7cebf902ec_z.jpg','9743494618_f0faea1cdc_z.jpg','8700354838_884967117d_z.jpg','9362545580_34d972390d_z.jpg']\n",
    "\n",
    "\n",
    "\n",
    "beam_sizes = [i for i in range(1,15)]\n",
    "beam_time = {}\n",
    "beam_score_rouge1_p = {}\n",
    "beam_score_rouge1_r = {}\n",
    "beam_score_rougeL_p = {}\n",
    "beam_score_rougeL_r = {}\n",
    "\n",
    "for i in beam_sizes:\n",
    "    from rouge_score import rouge_scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    AVG_ROUGE_1_precision=0\n",
    "    AVG_ROUGE_1_recall=0\n",
    "    AVG_ROUGE_L_precision=0\n",
    "    AVG_ROUGE_L_recall=0\n",
    "    ind = 0\n",
    "#     strings = []\n",
    "    time_taken = []\n",
    "    for image in image_paths:\n",
    "        tt,st = img_caption(\"./test/\"+image,i)\n",
    "#         strings.append(st)\n",
    "        time_taken.append(tt)\n",
    "        scores=scorer.score(ACTUALS[ind], st)\n",
    "        \n",
    "        AVG_ROUGE_1_precision+=scores['rouge1'].precision\n",
    "        AVG_ROUGE_L_precision+=scores['rougeL'].precision\n",
    "        AVG_ROUGE_1_recall+=scores['rouge1'].recall\n",
    "        AVG_ROUGE_L_recall+=scores['rougeL'].recall\n",
    "        \n",
    "        ind = ind + 1\n",
    "        \n",
    "    beam_time[i] = sum(time_taken) / len(time_taken)\n",
    "    beam_score_rouge1_p[i] = AVG_ROUGE_1_precision/10\n",
    "    beam_score_rouge1_r[i] = AVG_ROUGE_1_recall/10\n",
    "    beam_score_rougeL_p[i] = AVG_ROUGE_L_precision/10\n",
    "    beam_score_rougeL_r[i] = AVG_ROUGE_L_recall/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.32858026027679443,\n",
       " 2: 0.4674741506576538,\n",
       " 3: 0.48569068908691404,\n",
       " 4: 0.5796504735946655,\n",
       " 5: 0.7023421049118042,\n",
       " 6: 0.8307416915893555,\n",
       " 7: 1.030248498916626,\n",
       " 8: 1.0183224201202392,\n",
       " 9: 1.0985748291015625,\n",
       " 10: 1.265007448196411,\n",
       " 11: 1.3867156982421875,\n",
       " 12: 1.5191189765930175,\n",
       " 13: 1.5014585256576538,\n",
       " 14: 1.6311741113662719}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.4377777777777777,\n",
       " 2: 0.36477272727272725,\n",
       " 3: 0.35178571428571426,\n",
       " 4: 0.3575,\n",
       " 5: 0.3575,\n",
       " 6: 0.3575,\n",
       " 7: 0.3575,\n",
       " 8: 0.3575,\n",
       " 9: 0.3575,\n",
       " 10: 0.3575,\n",
       " 11: 0.3575,\n",
       " 12: 0.35083333333333333,\n",
       " 13: 0.35083333333333333,\n",
       " 14: 0.35083333333333333}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_score_rouge1_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.25505633255633253,\n",
       " 2: 0.23373348873348876,\n",
       " 3: 0.22373348873348875,\n",
       " 4: 0.22297591297591296,\n",
       " 5: 0.22297591297591296,\n",
       " 6: 0.22297591297591296,\n",
       " 7: 0.22297591297591296,\n",
       " 8: 0.22297591297591296,\n",
       " 9: 0.22297591297591296,\n",
       " 10: 0.22297591297591296,\n",
       " 11: 0.22297591297591296,\n",
       " 12: 0.22297591297591296,\n",
       " 13: 0.22297591297591296,\n",
       " 14: 0.22297591297591296}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_score_rouge1_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.4011111111111111,\n",
       " 2: 0.36477272727272725,\n",
       " 3: 0.35178571428571426,\n",
       " 4: 0.3575,\n",
       " 5: 0.3575,\n",
       " 6: 0.3575,\n",
       " 7: 0.3575,\n",
       " 8: 0.3575,\n",
       " 9: 0.3575,\n",
       " 10: 0.3575,\n",
       " 11: 0.3575,\n",
       " 12: 0.35083333333333333,\n",
       " 13: 0.35083333333333333,\n",
       " 14: 0.35083333333333333}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_score_rougeL_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.23763209013209016,\n",
       " 2: 0.23373348873348876,\n",
       " 3: 0.22373348873348875,\n",
       " 4: 0.22297591297591296,\n",
       " 5: 0.22297591297591296,\n",
       " 6: 0.22297591297591296,\n",
       " 7: 0.22297591297591296,\n",
       " 8: 0.22297591297591296,\n",
       " 9: 0.22297591297591296,\n",
       " 10: 0.22297591297591296,\n",
       " 11: 0.22297591297591296,\n",
       " 12: 0.22297591297591296,\n",
       " 13: 0.22297591297591296,\n",
       " 14: 0.22297591297591296}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_score_rougeL_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
