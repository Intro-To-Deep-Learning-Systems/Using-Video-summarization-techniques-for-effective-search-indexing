{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "iris = load_iris()\n",
    "logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
    "                               random_state=0)\n",
    "distributions = dict(C=uniform(loc=0, scale=4),\n",
    "                     penalty=['l2', 'l1'])\n",
    "clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
    "search = clf.fit(iris.data, iris.target)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './ExpansionNet/rf_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0r/095b68zx2zv0t279tgqqhl7w0000gn/T/ipykernel_27835/17483502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './ExpansionNet/rf_model.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import argparse\n",
    "import pickle\n",
    "from argparse import Namespace\n",
    "\n",
    "from PIL import Image as PIL_Image\n",
    "from ExpansionNet.models.End_ExpansionNet_v2 import End_ExpansionNet_v2\n",
    "from ExpansionNet.utils.language_utils import convert_vector_idx2word\n",
    "CPU = torch.device('cpu')\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "            N_dec:\n",
    "            3\n",
    "            N_enc:\n",
    "            3\n",
    "            beam_size:\n",
    "            5\n",
    "            image_paths:\n",
    "            ['./demo_material/tatin.jpg', './demo_material/micheal.jpg', './demo_material/napoleon.jpg', './demo_material/cat_girl.jpg']\n",
    "            load_path:\n",
    "            './rf_model.pth'\n",
    "            max_seq_len:\n",
    "            74\n",
    "            model_dim:\n",
    "            512\n",
    "    \n",
    "    '''\n",
    "    image_paths=['./nodeserver/pythonscripts/test/1176519574_b4d569ecbe_z.jpg','nodeserver/pythonscripts/test/2633528304_369cf89ce1_z.jpg']\n",
    "    drop_args = Namespace(enc=0.0,\n",
    "                          dec=0.0,\n",
    "                          enc_input=0.0,\n",
    "                          dec_input=0.0,\n",
    "                          other=0.0)\n",
    "    model_dim=512\n",
    "    N_enc=3\n",
    "    N_dec=3\n",
    "    max_seq_len=74\n",
    "    beam_size=5\n",
    "    load_path='./ExpansionNet/rf_model.pth'\n",
    "    model_args = Namespace(model_dim=model_dim,\n",
    "                           N_enc=N_enc,\n",
    "                           N_dec=N_dec,\n",
    "                           dropout=0.0,\n",
    "                           drop_args=drop_args)\n",
    "\n",
    "    with open('./ExpansionNet/demo_coco_tokens.pickle', 'rb') as f:\n",
    "        coco_tokens = pickle.load(f)\n",
    "    print(\"Dictionary loaded ...\")\n",
    "\n",
    "    img_size = 384\n",
    "    model = End_ExpansionNet_v2(swin_img_size=img_size, swin_patch_size=4, swin_in_chans=3,\n",
    "                                swin_embed_dim=192, swin_depths=[2, 2, 18, 2], swin_num_heads=[6, 12, 24, 48],\n",
    "                                swin_window_size=12, swin_mlp_ratio=4., swin_qkv_bias=True, swin_qk_scale=None,\n",
    "                                swin_drop_rate=0.0, swin_attn_drop_rate=0.0, swin_drop_path_rate=0.0,\n",
    "                                swin_norm_layer=torch.nn.LayerNorm, swin_ape=False, swin_patch_norm=True,\n",
    "                                swin_use_checkpoint=False,\n",
    "                                final_swin_dim=1536,\n",
    "\n",
    "                                d_model=model_args.model_dim, N_enc=model_args.N_enc,\n",
    "                                N_dec=model_args.N_dec, num_heads=8, ff=2048,\n",
    "                                num_exp_enc_list=[32, 64, 128, 256, 512],\n",
    "                                num_exp_dec=16,\n",
    "                                output_word2idx=coco_tokens['word2idx_dict'],\n",
    "                                output_idx2word=coco_tokens['idx2word_list'],\n",
    "                                max_seq_len=max_seq_len, drop_args=model_args.drop_args,\n",
    "                                rank=0)\n",
    "    model.to(CPU)#\n",
    "    map_location = {'cpu': 'cpu'}#\n",
    "    \n",
    "    checkpoint = torch.load(load_path, map_location=map_location)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(\"Model loaded ...\")\n",
    "\n",
    "    transf_1 = torchvision.transforms.Compose([torchvision.transforms.Resize((img_size, img_size))])\n",
    "    transf_2 = torchvision.transforms.Compose([torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                                std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "    input_images = []\n",
    "    for path in image_paths:\n",
    "        pil_image = PIL_Image.open(path)\n",
    "        if pil_image.mode != 'RGB':\n",
    "           pil_image = PIL_Image.new(\"RGB\", pil_image.size)\n",
    "        preprocess_pil_image = transf_1(pil_image)\n",
    "        tens_image_1 = torchvision.transforms.ToTensor()(preprocess_pil_image)\n",
    "        tens_image_2 = transf_2(tens_image_1)\n",
    "        input_images.append(tens_image_2)\n",
    "\n",
    "    print(\"Generating captions ...\\n\")\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(len(input_images)):\n",
    "        path = image_paths[i]\n",
    "        image = input_images[i].unsqueeze(0).to(CPU)\n",
    "        beam_search_kwargs = {'beam_size': beam_size,\n",
    "                              'beam_max_seq_len': max_seq_len,\n",
    "                              'sample_or_max': 'max',\n",
    "                              'how_many_outputs': 1,\n",
    "                              'sos_idx': coco_tokens['word2idx_dict'][coco_tokens['sos_str']],\n",
    "                              'eos_idx': coco_tokens['word2idx_dict'][coco_tokens['eos_str']]}\n",
    "        with torch.no_grad():\n",
    "            pred, _ = model(enc_x=image,\n",
    "                            enc_x_num_pads=[0],\n",
    "                            mode='beam_search', **beam_search_kwargs)\n",
    "        pred = convert_vector_idx2word(pred[0][0], coco_tokens['idx2word_list'])[1:-1]\n",
    "        pred[-1] = pred[-1] + '.'\n",
    "        pred = ' '.join(pred).capitalize()\n",
    "        print(path + ') \\n\\tDescription: ' + pred + '\\n')\n",
    "    end = time.time()\n",
    "    print(end-start)\n",
    "\n",
    "    print(\"Closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import os\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "import argparse\n",
    "from typing import Tuple, List, Union, Optional\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import skimage.io as io\n",
    "import PIL.Image\n",
    "from IPython.display import Image \n",
    "N = type(None)\n",
    "V = np.array\n",
    "ARRAY = np.ndarray\n",
    "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
    "VS = Union[Tuple[V, ...], List[V]]\n",
    "VN = Union[V, N]\n",
    "VNS = Union[VS, N]\n",
    "T = torch.Tensor\n",
    "TS = Union[Tuple[T, ...], List[T]]\n",
    "TN = Optional[T]\n",
    "TNS = Union[Tuple[TN, ...], List[TN]]\n",
    "TSN = Optional[TS]\n",
    "TA = Union[T, ARRAY]\n",
    "\n",
    "\n",
    "D = torch.device\n",
    "CPU = torch.device('cpu')\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def forward(self, x: T) -> T:\n",
    "        return self.model(x)\n",
    "\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) -1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    #@functools.lru_cache #FIXME\n",
    "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
    "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        if prefix_length > 10:  # not enough memory\n",
    "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
    "        else:\n",
    "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
    "\n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self\n",
    "\n",
    "\n",
    "#@title Caption prediction\n",
    "\n",
    "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
    "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
    "\n",
    "    model.eval()\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    tokens = None\n",
    "    scores = None\n",
    "    device = next(model.parameters()).device\n",
    "    seq_lengths = torch.ones(beam_size, device=device)\n",
    "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
    "    with torch.no_grad():\n",
    "        if embed is not None:\n",
    "            generated = embed\n",
    "        else:\n",
    "            if tokens is None:\n",
    "                tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                tokens = tokens.unsqueeze(0).to(device)\n",
    "                generated = model.gpt.transformer.wte(tokens)\n",
    "        for i in range(entry_length):\n",
    "            outputs = model.gpt(inputs_embeds=generated)\n",
    "            logits = outputs.logits\n",
    "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "            logits = logits.softmax(-1).log()\n",
    "            if scores is None:\n",
    "                scores, next_tokens = logits.topk(beam_size, -1)\n",
    "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
    "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
    "                if tokens is None:\n",
    "                    tokens = next_tokens\n",
    "                else:\n",
    "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
    "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "            else:\n",
    "                logits[is_stopped] = -float(np.inf)\n",
    "                logits[is_stopped, 0] = 0\n",
    "                scores_sum = scores[:, None] + logits\n",
    "                seq_lengths[~is_stopped] += 1\n",
    "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
    "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
    "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
    "                seq_lengths = seq_lengths[next_tokens_source]\n",
    "                next_tokens = next_tokens % scores_sum.shape[1]\n",
    "                next_tokens = next_tokens.unsqueeze(1)\n",
    "                tokens = tokens[next_tokens_source]\n",
    "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "                generated = generated[next_tokens_source]\n",
    "                scores = scores_sum_average * seq_lengths\n",
    "                is_stopped = is_stopped[next_tokens_source]\n",
    "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
    "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
    "            if is_stopped.all():\n",
    "                break\n",
    "    scores = scores / seq_lengths\n",
    "    output_list = tokens.cpu().numpy()\n",
    "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
    "    order = scores.argsort(descending=True)\n",
    "    output_texts = [output_texts[i] for i in order]\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "def generate2(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        tokens=None,\n",
    "        prompt=None,\n",
    "        embed=None,\n",
    "        entry_count=1,\n",
    "        entry_length=67,\n",
    "        top_p=0.8,\n",
    "        temperature=1.,\n",
    "        stop_token: str = '.',\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    filter_value = -float(\"Inf\")\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "            if embed is not None:\n",
    "                generated = embed\n",
    "            else:\n",
    "                if tokens is None:\n",
    "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                    tokens = tokens.unsqueeze(0).to(device)\n",
    "\n",
    "                generated = model.gpt.transformer.wte(tokens)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "\n",
    "                outputs = model.gpt(inputs_embeds=generated)\n",
    "                logits = outputs.logits\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                                                    ..., :-1\n",
    "                                                    ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
    "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
    "                if tokens is None:\n",
    "                    tokens = next_token\n",
    "                else:\n",
    "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
    "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "                if stop_token_index == next_token.item():\n",
    "                    break\n",
    "\n",
    "            output_list = list(tokens.squeeze().cpu().numpy())\n",
    "            output_text = tokenizer.decode(output_list)\n",
    "            generated_list.append(output_text)\n",
    "\n",
    "    return generated_list[0]\n",
    "\n",
    "\n",
    "def img_caption(filepath,beam_size):\n",
    "    print(filepath, file=sys.stderr)\n",
    "    def get_device(device_id: int) -> D:\n",
    "        if not torch.cuda.is_available():\n",
    "            return CPU\n",
    "        device_id = min(torch.cuda.device_count() - 1, device_id)\n",
    "        return torch.device(f'cuda:{device_id}')\n",
    "\n",
    "    is_gpu = True\n",
    "    CUDA = get_device\n",
    " #     model_path = os.path.join(os.getcwd(),\"nodeserver\",\"pythonscripts\",\"imagecaption\",\"model\", 'model_weights.pt')\n",
    "    model_path =\"./imagecaption/model/model_weights.pt\"\n",
    "    device = CUDA(0) if is_gpu else \"cpu\"\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    prefix_length = 10\n",
    "\n",
    "    model = ClipCaptionModel(prefix_length)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=CPU)) \n",
    "\n",
    "    model = model.eval() \n",
    "    device = CUDA(0) if is_gpu else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    use_beam_search = True  \n",
    "    image = io.imread(filepath)\n",
    "    pil_image = PIL.Image.fromarray(image)\n",
    "\n",
    "    # display(pil_image)\n",
    "    import time\n",
    "    start=time.time()\n",
    "    image = preprocess(pil_image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
    "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "    if use_beam_search:\n",
    "        generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed,beam_size = beam_size)[0]\n",
    "    else:\n",
    "        generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n",
    "\n",
    "    end=time.time()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     print('\\n')\n",
    "    print(\"Time Taken\",str(end-start), file=sys.stderr)\n",
    "\n",
    "    print(generated_text_prefix, file=sys.stderr)\n",
    "    return end-start,str(generated_text_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.36925172805786133\n",
      "elephants walking in the bush.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 0.5059001445770264\n",
      "tennis player celebrates after winning his match against tennis player during day.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.2357950210571289\n",
      "a city is a port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 0.485889196395874\n",
      "person, left, and person, right, look at a mirror.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.29430699348449707\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.28139495849609375\n",
      "a bus is loaded with passengers.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.2283341884613037\n",
      "the train arrives at station.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 0.31842613220214844\n",
      "a train passes through australian suburb.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.28358006477355957\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.28292417526245117\n",
      "airliner on final approach to land.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.5232357978820801\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 0.6714208126068115\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.25226402282714844\n",
      "a view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 0.6032328605651855\n",
      "person, left, and person look at each other in the bathroom.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.33213114738464355\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.3708047866821289\n",
      "people boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.2558720111846924\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 0.41803717613220215\n",
      "a train passes through australian suburb.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.3613309860229492\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.8864119052886963\n",
      "aircraft model is a twin - turboprop aircraft manufactured by aircraft manufacturer.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.4702467918395996\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 0.773474931716919\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.493664026260376\n",
      "a view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 0.6643319129943848\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.3516077995300293\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.4826171398162842\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.2799248695373535\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 0.5868620872497559\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.36072206497192383\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.39345526695251465\n",
      "aircraft model in flight at the airport.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.5232391357421875\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 0.951495885848999\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.5288188457489014\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 0.9980430603027344\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.4111459255218506\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.5759768486022949\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.32153820991516113\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 0.6189398765563965\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.3997318744659424\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.4675750732421875\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.7044563293457031\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 1.2808513641357422\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n",
      "Time Taken 0.6074860095977783\n",
      "aerial view of the port.\n",
      "./test/1176519574_b4d569ecbe_z.jpg\n",
      "Time Taken 1.0983312129974365\n",
      "person, left, and person look at each other in the locker room.\n",
      "./test/5093961450_d8c840d0d2_z.jpg\n",
      "Time Taken 0.45165181159973145\n",
      "cattle graze in a pasture.\n",
      "./test/9372536208_b97bc7eeff_z.jpg\n",
      "Time Taken 0.5871312618255615\n",
      "passengers boarding a bus at a bus station.\n",
      "./test/9419081044_7cebf902ec_z.jpg\n",
      "Time Taken 0.4036240577697754\n",
      "a train on the line.\n",
      "./test/9743494618_f0faea1cdc_z.jpg\n",
      "Time Taken 0.7728328704833984\n",
      "a train passes through australian local government area.\n",
      "./test/8700354838_884967117d_z.jpg\n",
      "Time Taken 0.5357658863067627\n",
      "a police officer on a scooter.\n",
      "./test/9362545580_34d972390d_z.jpg\n",
      "Time Taken 0.5812902450561523\n",
      "aircraft model on the runway.\n",
      "./test/5155093236_01067e4f6e_z.jpg\n",
      "Time Taken 0.8522889614105225\n",
      "elephants in the wild - photo #.\n",
      "./test/2633528304_369cf89ce1_z.jpg\n",
      "Time Taken 1.4636850357055664\n",
      "tennis player in action during his first round match against tennis player.\n",
      "./test/9874637964_ce338575f0_z.jpg\n"
     ]
    }
   ],
   "source": [
    "ACTUALS=[\"two elephants walking in a grassy flat area\",\n",
    "            \"a tennis player getting ready to hit a ball\",\n",
    "            \"a group of large boats is parts at a city marina\",\n",
    "            \"a man standing next to a man in a bathroom\",\n",
    "            \"a herd of cattle are standing and grazing in a field\",\n",
    "            \"men standing in a street next to a car and a large bus\",\n",
    "            \"a train moves past a cement station platform\",\n",
    "            \"a long railroad train carrying several cars behind it\",\n",
    "            \"a man on a motor bike on a street\",\n",
    "            \"a jumbo jet airplane coming in for a landing on a runway\"]\n",
    "\n",
    "image_paths=['5155093236_01067e4f6e_z.jpg','2633528304_369cf89ce1_z.jpg','9874637964_ce338575f0_z.jpg','1176519574_b4d569ecbe_z.jpg','5093961450_d8c840d0d2_z.jpg','9372536208_b97bc7eeff_z.jpg','9419081044_7cebf902ec_z.jpg','9743494618_f0faea1cdc_z.jpg','8700354838_884967117d_z.jpg','9362545580_34d972390d_z.jpg']\n",
    "\n",
    "\n",
    "\n",
    "beam_sizes = [i for i in range(1,15)]\n",
    "beam_time = {}\n",
    "beam_score_rouge1_p = {}\n",
    "beam_score_rouge1_r = {}\n",
    "beam_score_rougeL_p = {}\n",
    "beam_score_rougeL_r = {}\n",
    "\n",
    "for i in beam_sizes:\n",
    "    from rouge_score import rouge_scorer\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    AVG_ROUGE_1_precision=0\n",
    "    AVG_ROUGE_1_recall=0\n",
    "    AVG_ROUGE_L_precision=0\n",
    "    AVG_ROUGE_L_recall=0\n",
    "    ind = 0\n",
    "#     strings = []\n",
    "    time_taken = []\n",
    "    for image in image_paths:\n",
    "        tt,st = img_caption(\"./test/\"+image,i)\n",
    "#         strings.append(st)\n",
    "        time_taken.append(tt)\n",
    "        scores=scorer.score(ACTUALS[ind], st)\n",
    "        \n",
    "        AVG_ROUGE_1_precision+=scores['rouge1'].precision\n",
    "        AVG_ROUGE_L_precision+=scores['rougeL'].precision\n",
    "        AVG_ROUGE_1_recall+=scores['rouge1'].recall\n",
    "        AVG_ROUGE_L_recall+=scores['rougeL'].recall\n",
    "        \n",
    "        ind = ind + 1\n",
    "        \n",
    "    beam_time[i] = sum(time_taken) / len(time_taken)\n",
    "    beam_score_rouge1_p[i] = AVG_ROUGE_1_precision/10\n",
    "    beam_score_rouge1_r[i] = AVG_ROUGE_1_recall/10\n",
    "    beam_score_rougeL_p[i] = AVG_ROUGE_L_precision/10\n",
    "    beam_score_rougeL_r[i] = AVG_ROUGE_L_recall/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0.6821645259857178,\n",
       " 6: 0.8053165197372436,\n",
       " 7: 0.957831859588623,\n",
       " 8: 0.9760019063949585,\n",
       " 9: 1.036835789680481,\n",
       " 10: 1.2701971054077148,\n",
       " 11: 1.3266460657119752,\n",
       " 12: 1.421921968460083,\n",
       " 13: 1.516295313835144,\n",
       " 14: 1.5525250434875488,\n",
       " 15: 1.6323148727416992,\n",
       " 16: 1.9469746828079224,\n",
       " 17: 2.214304566383362,\n",
       " 18: 2.0027410268783568,\n",
       " 19: 2.277824378013611}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{5: 0.3575,\n",
       " 6: 0.7150000000000001,\n",
       " 7: 1.0725000000000002,\n",
       " 8: 1.4300000000000004,\n",
       " 9: 1.7875,\n",
       " 10: 2.1449999999999996,\n",
       " 11: 2.502499999999999,\n",
       " 12: 2.853333333333332,\n",
       " 13: 3.204166666666665,\n",
       " 14: 3.5549999999999984,\n",
       " 15: 3.9058333333333315,\n",
       " 16: 4.223333333333332,\n",
       " 17: 4.526590909090908,\n",
       " 18: 4.892186147186146,\n",
       " 19: 5.195443722943721}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
